{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE1Sg2ppRpc0"
      },
      "source": [
        "# LM Format Enforcer Integration with vLLM\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/noamgat/lm-format-enforcer/blob/main/samples/colab_vllm_integration.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "This notebook shows how you can integrate with the vLLM library.\n",
        "\n",
        "## Setting up the COLAB runtime (user action required)\n",
        "\n",
        "This colab-friendly notebook is targeted at demoing the enforcer on LLAMA2. It can run on a free GPU on Google Colab.\n",
        "Make sure that your runtime is set to GPU:\n",
        "\n",
        "Menu Bar -> Runtime -> Change runtime type -> T4 GPU (at the time of writing this notebook). [Guide here](https://www.codesansar.com/deep-learning/using-free-gpu-tpu-google-colab.htm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qiOgYRQRpc1"
      },
      "source": [
        "## Gathering huggingface credentials (user action required)\n",
        "\n",
        "We begin by installing the dependencies. This demo uses llama2, so you will have to create a free huggingface account, request access to the llama2 model, create an access token, and insert it when executing the next cell will request it.\n",
        "\n",
        "Links:\n",
        "\n",
        "- [Request access to llama model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf). See the \"Access Llama 2 on Hugging Face\" section.\n",
        "- [Create huggingface access token](https://huggingface.co/settings/tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U4F0IfkRpc1"
      },
      "outputs": [],
      "source": [
        "!pip install vllm lm-format-enforcer pandas\n",
        "\n",
        "# When running from source / developing the library, use this instead\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# import sys\n",
        "# import os\n",
        "# sys.path.append(os.path.abspath('..'))\n",
        "## os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJaxQ_q8Rpc2"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xjgU5OLRpc2"
      },
      "source": [
        "We load the model, as is normally done with vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1rxonYRRpc2"
      },
      "outputs": [],
      "source": [
        "import vllm\n",
        "model_id = 'google/gemma-3-1b-it'\n",
        "llm = vllm.LLM(model=model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSJ-ea5VRpc3"
      },
      "source": [
        "If the previous cell executed successfully, you have propertly set up your Colab runtime and huggingface account!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq_8SE_SRpc3"
      },
      "source": [
        "A few helper functions to make display nicer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oWd_OAIRpc3"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def display_header(text):\n",
        "    display(Markdown(f'**{text}**'))\n",
        "\n",
        "def display_content(text):\n",
        "    display(Markdown(f'```\\n{text}\\n```'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvDRcDcARpc3"
      },
      "source": [
        "## Setting up the prompt for the specific language model\n",
        "\n",
        "We set up the prompting style according to the [Llama2 demo](https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/app.py). We simplify the implementation a bit as we don't need chat history for this demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzxqf5L0Rpc3"
      },
      "outputs": [],
      "source": [
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
        "\"\"\"\n",
        "\n",
        "def get_prompt(message: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
        "    return f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{message} [/INST]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgl1aC7mRpc3"
      },
      "source": [
        "## Integrating CharacterLevelParser with vLLM\n",
        "\n",
        "We connect our parser to vLLM using the integration function `build_vllm_logits_processor()`.\n",
        "\n",
        "We then connect that processor to vLLM using the `SamplingParams.logits_processor` field.\n",
        "\n",
        "This is the ONLY required integration point between the two libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVuYOxD1Rpc3"
      },
      "outputs": [],
      "source": [
        "from lmformatenforcer import CharacterLevelParser\n",
        "from lmformatenforcer.integrations.vllm import build_vllm_logits_processor, build_vllm_token_enforcer_tokenizer_data\n",
        "from typing import Union, List, Optional\n",
        "from vllm import SamplingParams\n",
        "\n",
        "DEFAULT_MAX_NEW_TOKENS = 100\n",
        "\n",
        "ListOrStrList = Union[str, List[str]]\n",
        "\n",
        "tokenizer_data = build_vllm_token_enforcer_tokenizer_data(llm)\n",
        "\n",
        "def vllm_with_character_level_parser(prompt: ListOrStrList, parser: Optional[CharacterLevelParser] = None) -> ListOrStrList:\n",
        "\n",
        "    sampling_params = SamplingParams()\n",
        "    sampling_params.max_tokens = DEFAULT_MAX_NEW_TOKENS\n",
        "    if parser:\n",
        "        logits_processor = build_vllm_logits_processor(tokenizer_data, parser)\n",
        "        sampling_params.logits_processors = [logits_processor]\n",
        "    # Note on batched generation:\n",
        "    # For some reason, I achieved better batch performance by manually adding a loop similar to this:\n",
        "    # https://github.com/vllm-project/vllm/blob/main/examples/llm_engine_example.py,\n",
        "    # I don't know why this is faster than simply calling llm.generate() with a list of prompts, but it is from my tests.\n",
        "    # However, this demo focuses on simplicity, so I'm not including that here.\n",
        "    results = llm.generate(prompt, sampling_params=sampling_params)\n",
        "    if isinstance(prompt, str):\n",
        "        return results[0].outputs[0].text\n",
        "    else:\n",
        "        return [result.outputs[0].text for result in results]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8VbIdh4Rpc4"
      },
      "source": [
        "## vLLM + JSON Use case\n",
        "\n",
        "Now we demonstrate using ```JsonSchemaParser```. We create a pydantic model, generate the schema from it, and use that to enforce the format.\n",
        "The output will always be in a format that can be parsed by the parser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULwQP82lRpc4"
      },
      "outputs": [],
      "source": [
        "from lmformatenforcer import JsonSchemaParser\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class AnswerFormat(BaseModel):\n",
        "    first_name: str\n",
        "    last_name: str\n",
        "    year_of_birth: int\n",
        "    num_seasons_in_nba: int\n",
        "\n",
        "question = 'Please give me information about Michael Jordan. You MUST answer using the following json schema: '\n",
        "question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n",
        "prompt = get_prompt(question_with_schema)\n",
        "\n",
        "display_header(\"Prompt:\")\n",
        "display_content(prompt)\n",
        "\n",
        "display_header(\"Answer, With json schema enforcing:\")\n",
        "\n",
        "result = vllm_with_character_level_parser(prompt, JsonSchemaParser(AnswerFormat.schema()))\n",
        "display_content(result)\n",
        "\n",
        "display_header(\"Answer, Without json schema enforcing:\")\n",
        "result = vllm_with_character_level_parser(prompt, None)\n",
        "display_content(result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxUwsypVRpc4"
      },
      "source": [
        "As you can see, the enforced output matches the required schema, while the unenforced does not. We have successfully integrated with vLLM!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mZegl_XRpc4"
      },
      "source": [
        "## Batching example\n",
        "\n",
        "Now we demonstrate that the model can be used to generate text in batches. This is useful for generating text in parallel, which is much faster than generating text sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go4_fmWDRpc4"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "\n",
        "players = ['Michael Jordan', 'Tim Duncan', 'Larry Bird', 'Magic Johnson', 'Patrick Ewing',\n",
        "           'Hakeem Olajuwan', 'Nate Archibald', 'Charles Barkley', 'Bob Cousy', 'Clyde Drexler',\n",
        "           'Julius Erving', 'John Havlicek', 'Elvin Hayes', 'Jerry Lucas', 'Moses Malone',\n",
        "           'George Mikan', 'Bob Pettit', 'Oscar Robertson', 'Bill Russell', 'Dolph Schayes']\n",
        "prompts = []\n",
        "for player in players:\n",
        "    question = f'Please give me information about {player}. You MUST answer using the following json schema: '\n",
        "    question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n",
        "    prompt = get_prompt(question_with_schema)\n",
        "    prompts.append(prompt)\n",
        "\n",
        "start = time()\n",
        "one_player_result = vllm_with_character_level_parser(prompts[0], JsonSchemaParser(AnswerFormat.schema()))\n",
        "end = time()\n",
        "print(f'Time taken for 1 player: {end - start}s')\n",
        "display_content(one_player_result)\n",
        "\n",
        "start = time()\n",
        "all_results = vllm_with_character_level_parser(prompts[1:], JsonSchemaParser(AnswerFormat.schema()))\n",
        "end = time()\n",
        "print(f'Time taken for {len(prompts)-1} players: {end - start}. Time per player: {(end - start)/(len(prompts)-1)}')\n",
        "display_content(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1OflbZlRpc4"
      },
      "source": [
        "# Regular Expression + Analysis Example\n",
        "\n",
        "We now show two additional features: Regular Expression support and interference analysis.\n",
        "\n",
        "The code here is a bit lower level, as we need the `logits_processor` instance so we don't call the helper function `vllm_with_character_level_parser` that we created earlier in this notebook.\n",
        "\n",
        "Interference analysis allows us to see how much the format enforcer had to act, and what would be the probability of the selected tokens had the format enforcer not intervened. This can help you improve result quality by improving prompting and modelling to reduce the interference required. As a rule of thumb - the less interference the better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73Fc_1q9Rpc5"
      },
      "outputs": [],
      "source": [
        "from lmformatenforcer.regexparser import RegexParser\n",
        "import pandas as pd\n",
        "\n",
        "date_regex = r'(0?[1-9]|1[0-2])\\/(0?[1-9]|1\\d|2\\d|3[01])\\/(19|20)\\d{2}'\n",
        "answer_regex = ' In mm/dd/yyyy format, Michael Jordan was born in ' + date_regex\n",
        "parser = RegexParser(answer_regex)\n",
        "\n",
        "question = 'When was Michael Jordan Born? Please answer in mm/dd/yyyy format.'\n",
        "prompt = get_prompt(question)\n",
        "display_header(\"Prompt:\")\n",
        "display_content(prompt)\n",
        "\n",
        "# Note the analyze=True flag, which is will create an analyzer in the processor\n",
        "logits_processor = build_vllm_logits_processor(tokenizer_data, parser, analyze=True)\n",
        "\n",
        "sampling_params = SamplingParams(max_tokens=200, logits_processors=[logits_processor])\n",
        "results = llm.generate(prompt, sampling_params=sampling_params)\n",
        "\n",
        "text = results[0].outputs[0].text\n",
        "display_header(\"Answer:\")\n",
        "display_content(text)\n",
        "\n",
        "display_header(\"Analyzer Results:\")\n",
        "report_dict = logits_processor.analyzer.generate_report_dict(results[0].outputs[0].token_ids)\n",
        "enforced_scores = pd.DataFrame(report_dict)\n",
        "# Setting some display options for readability\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_columns', 10)\n",
        "pd.set_option('display.max_rows', 999)\n",
        "pd.set_option('display.float_format', ' {:,.5f}'.format)\n",
        "display(enforced_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E-YiXzyRpc5"
      },
      "source": [
        "The timesteps in which `generated_score < leading_score` are those in which the format enforcer had to intervene. Consider using this during development to fine tune your prompts for better consistency.\n",
        "\n",
        "This method also works for JSON Schema mode, of course."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}